{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac2058a2-79b6-44f1-a60f-370db7bdba7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import trimesh\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import open3d as o3d\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from utils.data_load import load_image, load_3d_model, visualize_data # our own utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73be0b68-4a82-4759-a743-cf216313f71c",
   "metadata": {},
   "source": [
    "# Get main data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "387b4f33-f397-40cb-8e2d-5cf6713fc27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"data/pix3d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1917946c-241b-467c-a190-b9a5ca3ac81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{dir}/pix3d.json\", \"rb\") as f:\n",
    "    metadata = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd048ed1-00b7-486d-9a3d-3b16cfcc76f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "155e8eb9-8b9d-402d-9685-160646cabcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, masks, models, categories = [], [], [], []\n",
    "\n",
    "np.random.shuffle(metadata)\n",
    "\n",
    "for i in range(len(metadata))[:100]:\n",
    "    img_path = dir+'/'+metadata[i][\"img\"]\n",
    "    mask_path = dir+'/'+metadata[i][\"mask\"]\n",
    "    model_path = dir+'/'+metadata[i][\"model\"]\n",
    "\n",
    "    # take actual img, mask and model\n",
    "    img, mask = load_image(img_path, mask_path, (224,224))\n",
    "    model_img = load_3d_model(model_path)\n",
    "\n",
    "    # combine all geometries, if this is a scene\n",
    "    if isinstance(model_img, trimesh.Scene):\n",
    "        model_img = model_img.to_geometry()\n",
    "    else:\n",
    "        model_img = model_img\n",
    "\n",
    "    point_cloud, _ = trimesh.sample.sample_surface(model_img, count=1024)\n",
    "    # pcd = o3d.geometry.PointCloud()\n",
    "    # pcd.points = o3d.utility.Vector3dVector(point_cloud)\n",
    "\n",
    "    imgs.append(img)\n",
    "    masks.append(mask)\n",
    "    models.append(transform(point_cloud))\n",
    "    categories.append(metadata[i][\"category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5165766-c62e-4caf-9648-7146888b6991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57ce2981-9d2e-4345-b9c1-fc3d946ca2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = LabelEncoder().fit_transform(categories)\n",
    "n_categories = len(set(categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60409376-08a7-4b36-b805-81477acf57e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f643c250-0848-4095-b936-3ec3f559a675",
   "metadata": {},
   "source": [
    "# Analyze and preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa7fed5f-6d29-40fe-911c-0412ea9c35bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 224, 224])\n",
      "torch.Size([1, 1024, 3])\n"
     ]
    }
   ],
   "source": [
    "print(imgs[0].shape)\n",
    "print(masks[0].shape)\n",
    "print(models[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f9a281b-74c0-4f2e-ba12-c01573b0477f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(imgs[0]))\n",
    "print(type(masks[0]))\n",
    "print(type(models[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1ad126-ff33-4e35-8e1c-ea6a6c382f88",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c21990-80d6-4cb3-be5d-11d4db94590f",
   "metadata": {},
   "source": [
    "### Data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06f03aab-d3ba-4d89-b62c-3a18bb56b3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class PointCloudDataset(Dataset):\n",
    "    def __init__(self, images, depth_maps, point_clouds, categories, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images (list of tensors): List of image tensors.\n",
    "            depth_maps (list of tensors): List of depth map tensors.\n",
    "            point_clouds (list of tensors): List of point cloud tensors.\n",
    "            transform (callable, optional): Optional transform to be applied on the input data.\n",
    "        \"\"\"\n",
    "        self.images = images\n",
    "        self.depth_maps = depth_maps\n",
    "        self.point_clouds = point_clouds\n",
    "        self.categories = categories\n",
    "        self.transform = transform\n",
    "        \n",
    "        assert len(images) == len(depth_maps) == len(point_clouds), \\\n",
    "            \"Images, depth maps, and point clouds lists must have the same length\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Returns the number of samples in the dataset\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get image, depth map, and point cloud at the given index\n",
    "        image = self.images[idx]\n",
    "        depth_map = self.depth_maps[idx]\n",
    "        point_cloud = self.point_clouds[idx]\n",
    "        category = self.categories[idx]\n",
    "        \n",
    "        # Apply any transforms (e.g., data augmentation)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            depth_map = self.transform(depth_map)\n",
    "        \n",
    "        # Return a dictionary of the inputs and target (point cloud)\n",
    "        return {\n",
    "            'image': image,\n",
    "            'mask': depth_map,\n",
    "            'model': point_cloud,\n",
    "            'category': category\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3c755f2-1c76-4cfb-b127-077c8f303d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "dataset = PointCloudDataset(imgs, masks, models, categories)\n",
    "\n",
    "# Create DataLoader for batching\n",
    "data_loader = DataLoader(dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc95e20-f487-4a10-a547-a81878f70a69",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e41109ea-0a9f-436a-8276-f97c69a96d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ImageDepthEncoder(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(ImageDepthEncoder, self).__init__()\n",
    "        \n",
    "#         # Image Encoder (ResNet-like)\n",
    "#         self.image_conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "#         self.image_bn1 = nn.BatchNorm2d(64)\n",
    "#         self.image_conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "#         self.image_bn2 = nn.BatchNorm2d(128)\n",
    "#         self.image_conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
    "#         self.image_bn3 = nn.BatchNorm2d(256)\n",
    "        \n",
    "#     def forward(self, image):\n",
    "#         img_feat = F.relu(self.image_bn1(self.image_conv1(image)))\n",
    "#         img_feat = F.relu(self.image_bn2(self.image_conv2(img_feat)))\n",
    "#         img_feat = F.relu(self.image_bn3(self.image_conv3(img_feat)))\n",
    "\n",
    "#         return img_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6ba3f42-e178-4cb5-90a6-f5c2239e2fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PointCloudDecoder(nn.Module):\n",
    "#     def __init__(self, in_channels, num_points=1024):\n",
    "#         super(PointCloudDecoder, self).__init__()\n",
    "        \n",
    "#         # Decoder with ConvTranspose layers to upsample and decode into a point cloud\n",
    "#         self.conv1 = nn.ConvTranspose2d(in_channels, 256, kernel_size=4, stride=2, padding=1)\n",
    "#         self.conv2 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1)\n",
    "#         self.conv3 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)\n",
    "#         self.conv4 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1)\n",
    "        \n",
    "#         # Final layer to output (num_points * 3) channels for the 3D coordinates\n",
    "#         self.conv_final = nn.Conv2d(256, num_points * 3, kernel_size=3, padding=1)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # Upsample with ConvTranspose layers\n",
    "#         x = F.relu(self.conv1(x))\n",
    "#         print(\"Decoding after first layer -->\", x.shape)\n",
    "#         x = F.relu(self.conv2(x))\n",
    "#         print(\"Decoding after second layer -->\", x.shape)\n",
    "#         x = F.relu(self.conv3(x))\n",
    "#         print(\"Decoding after third layer -->\", x.shape)\n",
    "#         x = F.relu(self.conv4(x))\n",
    "#         print(\"Decoding after fourth layer -->\", x.shape)\n",
    "        \n",
    "#         # Final point cloud prediction\n",
    "#         point_cloud = self.conv_final(x)  # Output shape: (batch_size, num_points*3, H, W)\n",
    "        \n",
    "#         # Reshape to (batch_size, num_points, 3) for point cloud output\n",
    "#         point_cloud = point_cloud.view(point_cloud.size(0), -1, 3)\n",
    "        \n",
    "#         return point_cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d91bca2e-c6b5-4b19-a490-17a7f75feac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PointCloudPredictor(nn.Module):\n",
    "#     def __init__(self, num_points=1024):\n",
    "#         super(PointCloudPredictor, self).__init__()\n",
    "#         self.encoder = ImageDepthEncoder()\n",
    "#         self.decoder = PointCloudDecoder(in_channels=256, num_points=num_points)  # Combined feature dim from encoder\n",
    "    \n",
    "#     def forward(self, image, depth):\n",
    "#         # Encode image and depth features\n",
    "#         combined_features = self.encoder(image)\n",
    "#         print(\"After encoding -->\", combined_features.shape)\n",
    "        \n",
    "#         # Decode features into point cloud\n",
    "#         point_cloud = self.decoder(combined_features)\n",
    "#         print(\"After decoding -->\", point_cloud.shape)\n",
    "#         return point_cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c651f77-e03b-47d8-9efe-1b66f680f381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = PointCloudPredictor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cd6b9e-b546-4605-901e-7d1a25358c00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d41ab5e-5e87-4dde-92ba-e99e22703279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Encoder(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Encoder, self).__init__()\n",
    "#         # Convolutional layers for encoding\n",
    "#         self.conv1 = nn.Conv2d(4, 64, kernel_size=3, stride=2, padding=1)  # Input: (4, 256, 256), Output: (64, 128, 128)\n",
    "#         self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)  # Output: (128, 64, 64)\n",
    "#         self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)  # Output: (256, 32, 32)\n",
    "#         self.conv4 = nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1)  # Output: (256, 16, 16)\n",
    "#         self.conv5 = nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1)  # Output: (256, 8, 8)\n",
    "#         self.conv6 = nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1)  # Output: (256, 4, 4)\n",
    "#         self.conv7 = nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1)  # Output: (256, 2, 2)\n",
    "#         self.conv8 = nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1)  # Output: (256, 1, 1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.conv1(x))\n",
    "#         x = F.relu(self.conv2(x))\n",
    "#         x = F.relu(self.conv3(x))\n",
    "#         x = F.relu(self.conv4(x))\n",
    "#         x = F.relu(self.conv5(x))\n",
    "#         x = F.relu(self.conv6(x))\n",
    "#         x = F.relu(self.conv7(x))\n",
    "#         x = F.relu(self.conv8(x))\n",
    "#         return x  # Shape: (B, 256, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e3b937f-a5fb-46e2-91f8-13a9dbc51701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Decoder(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Decoder, self).__init__()\n",
    "#         # Fully connected layers to transform the encoded feature map to a point cloud\n",
    "#         self.fc1 = nn.Linear(256, 1024 * 3)  # Transform from 256 -> 1024*3\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.view(x.size(0), -1)  # Flatten (B, 256, 1, 1) -> (B, 256)\n",
    "#         x = F.relu(self.fc1(x))  # Fully connected layer to get (B, 1024*3)\n",
    "#         x = x.view(-1, 1024, 3)  # Reshape to (B, 1024, 3)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62508820-06a1-4bd5-8481-6279dbd58e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PointCloudPredictionNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(PointCloudPredictionNet, self).__init__()\n",
    "#         self.encoder = Encoder()\n",
    "#         self.decoder = Decoder()\n",
    "\n",
    "#     def forward(self, image):\n",
    "#         encoded = self.encoder(x)\n",
    "#         point_cloud = self.decoder(encoded)\n",
    "#         return point_cloud  # Output shape: (B, 1024, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc43a9fc-63fa-4551-b0e7-f970626b8f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = PointCloudPredictionNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c546af0-805a-48a8-a6ce-4a98b747163b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da75fe40-c819-4768-a9fc-4b571010ec64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFeatureNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageFeatureNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5, stride=2, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=5, stride=2, padding=2)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.fc1 = nn.Linear(256 * 28 * 28, 256)  # Adjust based on output size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))  # Output: (batch_size, 64, 112, 112)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))  # Output: (batch_size, 128, 56, 56)\n",
    "        x = F.relu(self.bn3(self.conv3(x)))  # Output: (batch_size, 256, 28, 28)\n",
    "        x = x.view(x.size(0), -1)  # Flatten: (batch_size, 256 * 28 * 28)\n",
    "        x = F.relu(self.fc1(x))  # Output: (batch_size, 256)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2bfe3171-c415-4188-bd4e-102d590d7343",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointCloudDecoder(nn.Module):\n",
    "    def __init__(self, num_points=1024):\n",
    "        super(PointCloudDecoder, self).__init__()\n",
    "        self.num_points = num_points\n",
    "        self.fc1 = nn.Linear(256, 512)\n",
    "        self.fc2 = nn.Linear(512, 1024)\n",
    "        self.fc3 = nn.Linear(1024, num_points * 3)  # Predict 3D coordinates (x, y, z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))  # Shape: (batch_size, 512)\n",
    "        x = F.relu(self.fc2(x))  # Shape: (batch_size, 1024)\n",
    "        x = self.fc3(x)  # Shape: (batch_size, num_points * 3)\n",
    "        x = x.view(-1, self.num_points, 3)  # Reshape to (batch_size, num_points, 3)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3312819-1332-431f-a5c8-001f1f66a19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullModel(nn.Module):\n",
    "    def __init__(self, num_classes, num_points=1024):\n",
    "        super(FullModel, self).__init__()\n",
    "        self.image_feature_net = ImageFeatureNet()\n",
    "        self.classification_fc = nn.Linear(256, num_classes)  # Classification branch\n",
    "        self.point_cloud_decoder = PointCloudDecoder(num_points=num_points)  # Point cloud reconstruction branch\n",
    "\n",
    "    def forward(self, image):\n",
    "        # Extract image features\n",
    "        image_features = self.image_feature_net(image)  # Shape: (batch_size, 256)\n",
    "\n",
    "        # Classification branch\n",
    "        class_output = self.classification_fc(image_features)  # Shape: (batch_size, num_classes)\n",
    "\n",
    "        # Point cloud reconstruction branch\n",
    "        point_cloud_output = self.point_cloud_decoder(image_features)  # Shape: (batch_size, num_points, 3)\n",
    "\n",
    "        return class_output, point_cloud_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7245d865-5917-4651-ad75-219c745d2eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FullModel(num_classes=n_categories, num_points=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d538944-ce9f-480e-a9e1-f14c29879ba2",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b8b2e39-3458-4d93-abbb-d293be02636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e483609-60e5-4a45-b0f2-7258b2ae0976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FullModel(\n",
       "  (image_feature_net): ImageFeatureNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3): Conv2d(128, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (fc1): Linear(in_features=200704, out_features=256, bias=True)\n",
       "  )\n",
       "  (classification_fc): Linear(in_features=256, out_features=8, bias=True)\n",
       "  (point_cloud_decoder): PointCloudDecoder(\n",
       "    (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (fc2): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    (fc3): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "197ad209-1a2d-4803-981c-857bc14ed47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in data_loader:\n",
    "        images_batch = batch['image']\n",
    "        depth_maps_batch = batch['mask']\n",
    "        point_clouds_batch = batch['model']\n",
    "        category = batch['category']\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        class_output, point_cloud_output = model(images_batch)\n",
    "        \n",
    "        loss = criterion(class_output, category).float()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883c0541-56e4-4e3f-a9c7-44f309199ced",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f796b034-5039-4a55-9976-bee8dcfaff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visuzlie_point_cloud(data):\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(point_cloud_example)\n",
    "    \n",
    "    o3d.visualization.draw_geometries([pcd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "707a4c4a-1b64-44dd-b966-fb817d6bc65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "point_cloud_example = point_clouds_batch[0].cpu().numpy()[-1, :, :]\n",
    "point_cloud_output_exampole = point_cloud_output[0].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f151c641-5aee-48fe-8920-8837c9b8a746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visuzlie_point_cloud(point_cloud_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "90117fc7-a62c-499a-9538-d737f44a0688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visuzlie_point_cloud(point_cloud_output_exampole)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
