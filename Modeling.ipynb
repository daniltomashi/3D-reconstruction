{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac2058a2-79b6-44f1-a60f-370db7bdba7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import trimesh\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import open3d as o3d\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from utils.data_load import load_image, load_3d_model, visualize_data # our own utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784b416b-0ec8-4c0b-9739-cb1c6a6cf7ca",
   "metadata": {},
   "source": [
    "# Functions for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "764ca9e8-45bc-40b9-aa74-7dc35f49e28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chamfer_distance(point_cloud1, point_cloud2):\n",
    "    \"\"\"\n",
    "        Chamfer Distance between two point clouds.\n",
    "    \"\"\"\n",
    "    \n",
    "    B, N, D = point_cloud1.size()\n",
    "    _, M, _ = point_cloud2.size()\n",
    "\n",
    "    point_cloud1 = point_cloud1.unsqueeze(2).expand(B, N, M, D)\n",
    "    point_cloud2 = point_cloud2.unsqueeze(1).expand(B, N, M, D)\n",
    "    \n",
    "    dist = torch.norm(point_cloud1 - point_cloud2, dim=3)  # Euclidean distance between points\n",
    "    \n",
    "    # Compute Chamfer Distance\n",
    "    min_dist1, _ = torch.min(dist, dim=2)  # Closest point from pc1 to pc2\n",
    "    min_dist2, _ = torch.min(dist, dim=1)  # Closest point from pc2 to pc1\n",
    "\n",
    "    # calculates mean per batch\n",
    "    chamfer = torch.mean(min_dist1, dim=1) + torch.mean(min_dist2, dim=1)\n",
    "    \n",
    "    return torch.mean(chamfer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73be0b68-4a82-4759-a743-cf216313f71c",
   "metadata": {},
   "source": [
    "# Get main data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "387b4f33-f397-40cb-8e2d-5cf6713fc27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"data/pix3d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1917946c-241b-467c-a190-b9a5ca3ac81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{dir}/pix3d.json\", \"rb\") as f:\n",
    "    metadata = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd048ed1-00b7-486d-9a3d-3b16cfcc76f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "155e8eb9-8b9d-402d-9685-160646cabcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, masks, models, categories = [], [], [], []\n",
    "\n",
    "np.random.shuffle(metadata)\n",
    "\n",
    "for i in range(len(metadata))[:500]:\n",
    "    img_path = dir+'/'+metadata[i][\"img\"]\n",
    "    mask_path = dir+'/'+metadata[i][\"mask\"]\n",
    "    model_path = dir+'/'+metadata[i][\"model\"]\n",
    "\n",
    "    # take actual img, mask and model\n",
    "    img, mask = load_image(img_path, mask_path, (224,224))\n",
    "    model_img = load_3d_model(model_path)\n",
    "\n",
    "    # combine all geometries, if this is a scene\n",
    "    if isinstance(model_img, trimesh.Scene):\n",
    "        model_img = model_img.to_geometry()\n",
    "    else:\n",
    "        model_img = model_img\n",
    "\n",
    "    point_cloud, _ = trimesh.sample.sample_surface(model_img, count=1024)\n",
    "    # pcd = o3d.geometry.PointCloud()\n",
    "    # pcd.points = o3d.utility.Vector3dVector(point_cloud)\n",
    "\n",
    "    imgs.append(img)\n",
    "    masks.append(mask)\n",
    "    models.append(transform(point_cloud))\n",
    "    categories.append(metadata[i][\"category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5165766-c62e-4caf-9648-7146888b6991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57ce2981-9d2e-4345-b9c1-fc3d946ca2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = LabelEncoder().fit_transform(categories)\n",
    "n_categories = len(set(categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60409376-08a7-4b36-b805-81477acf57e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "399abdc7-bab4-430a-bc2a-a00652aede34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train, test and validation\n",
    "train, test, validation = (imgs[:300], masks[:300], models[:300], categories[:300]),\\\n",
    "                          (imgs[300:400], masks[300:400], models[300:400], categories[300:400]),\\\n",
    "                          (imgs[400:500], masks[400:500], models[400:500], categories[400:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f643c250-0848-4095-b936-3ec3f559a675",
   "metadata": {},
   "source": [
    "# Analyze and preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa7fed5f-6d29-40fe-911c-0412ea9c35bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 224, 224])\n",
      "torch.Size([1, 1024, 3])\n"
     ]
    }
   ],
   "source": [
    "print(imgs[0].shape)\n",
    "print(masks[0].shape)\n",
    "print(models[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1ad126-ff33-4e35-8e1c-ea6a6c382f88",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c21990-80d6-4cb3-be5d-11d4db94590f",
   "metadata": {},
   "source": [
    "### Data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06f03aab-d3ba-4d89-b62c-3a18bb56b3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class PointCloudDataset(Dataset):\n",
    "    def __init__(self, images, depth_maps, point_clouds, categories, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images (list of tensors): List of image tensors.\n",
    "            depth_maps (list of tensors): List of depth map tensors.\n",
    "            point_clouds (list of tensors): List of point cloud tensors.\n",
    "            transform (callable, optional): Optional transform to be applied on the input data.\n",
    "        \"\"\"\n",
    "        self.images = images\n",
    "        self.depth_maps = depth_maps\n",
    "        self.point_clouds = point_clouds\n",
    "        self.categories = categories\n",
    "        self.transform = transform\n",
    "        \n",
    "        assert len(images) == len(depth_maps) == len(point_clouds), \\\n",
    "            \"Images, depth maps, and point clouds lists must have the same length\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Returns the number of samples in the dataset\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get image, depth map, and point cloud at the given index\n",
    "        image = self.images[idx]\n",
    "        depth_map = self.depth_maps[idx]\n",
    "        point_cloud = self.point_clouds[idx]\n",
    "        category = self.categories[idx]\n",
    "        \n",
    "        # Apply any transforms (e.g., data augmentation)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            depth_map = self.transform(depth_map)\n",
    "        \n",
    "        # Return a dictionary of the inputs and target (point cloud)\n",
    "        return {\n",
    "            'image': image,\n",
    "            'mask': depth_map,\n",
    "            'model': point_cloud,\n",
    "            'category': category\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3c755f2-1c76-4cfb-b127-077c8f303d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create dataset\n",
    "# dataset = PointCloudDataset(imgs, masks, models, categories)\n",
    "\n",
    "# # Create DataLoader for batching\n",
    "# data_loader = DataLoader(dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b63be112-92fe-4e6b-91d9-2c5f5576abad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PointCloudDataset(*train)\n",
    "test_dataset = PointCloudDataset(*test)\n",
    "validation_dataset = PointCloudDataset(*validation)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc95e20-f487-4a10-a547-a81878f70a69",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da75fe40-c819-4768-a9fc-4b571010ec64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFeatureNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageFeatureNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5, stride=2, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=5, stride=2, padding=2)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.fc1 = nn.Linear(256 * 28 * 28, 256)  # Adjust based on output size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))  # Output: (batch_size, 64, 112, 112)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))  # Output: (batch_size, 128, 56, 56)\n",
    "        x = F.relu(self.bn3(self.conv3(x)))  # Output: (batch_size, 256, 28, 28)\n",
    "        x = x.view(x.size(0), -1)  # Flatten: (batch_size, 256 * 28 * 28)\n",
    "        x = F.relu(self.fc1(x))  # Output: (batch_size, 256)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2bfe3171-c415-4188-bd4e-102d590d7343",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointCloudDecoder(nn.Module):\n",
    "    def __init__(self, num_points=1024):\n",
    "        super(PointCloudDecoder, self).__init__()\n",
    "        self.num_points = num_points\n",
    "        self.fc1 = nn.Linear(256, 512)\n",
    "        self.fc2 = nn.Linear(512, 1024)\n",
    "        self.fc3 = nn.Linear(1024, num_points * 3)  # Predict 3D coordinates (x, y, z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))  # Shape: (batch_size, 512)\n",
    "        x = F.relu(self.fc2(x))  # Shape: (batch_size, 1024)\n",
    "        x = self.fc3(x)  # Shape: (batch_size, num_points * 3)\n",
    "        x = x.view(-1, self.num_points, 3)  # Reshape to (batch_size, num_points, 3)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3312819-1332-431f-a5c8-001f1f66a19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullModel(nn.Module):\n",
    "    def __init__(self, num_classes, num_points=1024):\n",
    "        super(FullModel, self).__init__()\n",
    "        self.image_feature_net = ImageFeatureNet()\n",
    "        self.classification_fc = nn.Linear(256, num_classes)  # Classification branch\n",
    "        self.point_cloud_decoder = PointCloudDecoder(num_points=num_points)  # Point cloud reconstruction branch\n",
    "\n",
    "    def forward(self, image):\n",
    "        # Extract image features\n",
    "        image_features = self.image_feature_net(image)  # Shape: (batch_size, 256)\n",
    "\n",
    "        # Classification branch\n",
    "        class_output = self.classification_fc(image_features)  # Shape: (batch_size, num_classes)\n",
    "\n",
    "        # Point cloud reconstruction branch\n",
    "        point_cloud_output = self.point_cloud_decoder(image_features)  # Shape: (batch_size, num_points, 3)\n",
    "\n",
    "        return class_output, point_cloud_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7245d865-5917-4651-ad75-219c745d2eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FullModel(num_classes=n_categories, num_points=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d538944-ce9f-480e-a9e1-f14c29879ba2",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b8b2e39-3458-4d93-abbb-d293be02636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e483609-60e5-4a45-b0f2-7258b2ae0976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FullModel(\n",
       "  (image_feature_net): ImageFeatureNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3): Conv2d(128, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (fc1): Linear(in_features=200704, out_features=256, bias=True)\n",
       "  )\n",
       "  (classification_fc): Linear(in_features=256, out_features=9, bias=True)\n",
       "  (point_cloud_decoder): PointCloudDecoder(\n",
       "    (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (fc2): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    (fc3): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fae488d1-2446-4ccd-8997-81395530460f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "train_losses, validation_losses = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "197ad209-1a2d-4803-981c-857bc14ed47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 10/10 [00:44<00:00,  4.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs 1/2 - Train Classification Loss: 43.65836006164551, Validation Classification Loss: 5.882677459716797\n",
      "\t\tTrain Chamfer Distance: 4.569501224642344, Validation Chamfer Distance: 1.0721530884730393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 10/10 [00:49<00:00,  4.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs 2/2 - Train Classification Loss: 9.219881261189778, Validation Classification Loss: 3.548175086975098\n",
      "\t\tTrain Chamfer Distance: 2.488585152316124, Validation Chamfer Distance: 0.5453386166262097\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    point_clouds_train_loss, point_clouds_validation_loss = 0.0, 0.0\n",
    "    \n",
    "    for batch in tqdm(train_loader):\n",
    "        images_batch = batch['image']\n",
    "        depth_maps_batch = batch['mask']\n",
    "        point_clouds_batch = batch['model']\n",
    "        category = batch['category']\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        class_output, point_cloud_output = model(images_batch)\n",
    "        \n",
    "        loss = criterion(class_output, category).float()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images_batch.size(0)\n",
    "\n",
    "        # compute loss for point cloud prediction\n",
    "        chamfer_distance_train = chamfer_distance(point_cloud_output, point_clouds_batch[:, 0, :])\n",
    "\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # Validate at each epoch\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_loader:\n",
    "            images_batch = batch['image']\n",
    "            depth_maps_batch = batch['mask']\n",
    "            point_clouds_batch = batch['model']\n",
    "            category = batch['category']\n",
    "        \n",
    "            class_output, point_cloud_output = model(images_batch)\n",
    "            loss = criterion(class_output, category)\n",
    "            \n",
    "            running_loss += loss.item() * images_batch.size(0)\n",
    "\n",
    "            # compute loss for point cloud prediction\n",
    "            chamfer_distance_validation = chamfer_distance(point_cloud_output, point_clouds_batch[:, 0, :])\n",
    "\n",
    "    validation_loss = running_loss / len(validation_loader.dataset)\n",
    "    validation_losses.append(validation_loss)\n",
    "\n",
    "    print(f\"Epochs {epoch+1}/{num_epochs} - Train Classification Loss: {train_loss}, Validation Classification Loss: {validation_loss}\")\n",
    "    print(f\"\\t\\tTrain Chamfer Distance: {chamfer_distance_train}, Validation Chamfer Distance: {chamfer_distance_validation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36cd952-1682-46fe-8f01-da7547644ef2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f796b034-5039-4a55-9976-bee8dcfaff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visuzlie_point_cloud(data):\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(point_cloud_example)\n",
    "    \n",
    "    o3d.visualization.draw_geometries([pcd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "707a4c4a-1b64-44dd-b966-fb817d6bc65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "point_cloud_example = point_clouds_batch[0].cpu().numpy()[-1, :, :]\n",
    "point_cloud_output_exampole = point_cloud_output[0].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f151c641-5aee-48fe-8920-8837c9b8a746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visuzlie_point_cloud(point_cloud_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "90117fc7-a62c-499a-9538-d737f44a0688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visuzlie_point_cloud(point_cloud_output_exampole)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
